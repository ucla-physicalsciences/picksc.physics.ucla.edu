---
layout: default
---

    <!-- ======= Breadcrumbs ======= -->
    <section id="breadcrumbs" class="breadcrumbs">
      <div class="container">

        <div class="d-flex justify-content-between align-items-center">
          <h2>Open MPI-MPI</h2>
          <ol>
            <li><a href="index.html">Home</a></li>
            <li><a href="skeleton-code.html">Skeleton Code</a></li>
            <li>Open MP-MPI</li>
          </ol>
        </div>

      </div>
    </section><!-- End Breadcrumbs -->

    <!-- ======= Blog Single Section ======= -->
    <section id="blog" class="blog">
      <div class="container" data-aos="fade-up">

        <div class="row">

          <div class="col-lg-8 entries">

            <article class="entry entry-single">

              <!--<div class="entry-img">
                <img src="assets/img/secondary-sidebar.jpg" alt="" class="img-fluid">
              </div>-->

              <!--<h2 class="entry-title">
                <a href="blog-single.html">Dolorum optio tempore voluptas dignissimos cumque fuga qui quibusdam quia</a>
              </h2>-->

              <div class="entry-content">
              <p>These codes illustrate how to use a hybrid shared/distributed memory algorithm, with a tiled scheme on each 
              shared memory multi-core node implemented with OpenMP, and domain decomposition connecting such nodes implemented 
              with MPI. The algorithms are described in detail in Refs. [2-4].</p>
              <p>For the 2D electrostatic code, a typical execution time for the particle part of this code is about 80 
              ps/particle/time-step with 576 processing cores. For the 2-1/2D electromagnetic code, a typical execution time 
              for the particle part of this code is about 230 ps/particle/time-step with 576 processing cores. For the 2-1/2D 
              Darwin code, a typical execution time for the particle part of this code is about 364 ps/particle/time-step with 
              576 processing cores. The CPUs (2.67GHz Intel Nehalem processors) were throttled down to 1.6 GHz for these 
              benchmarks.</p>
              <p>For the 3D electrostatic code, a typical execution time for the particle part of this code is about 110 
              ps/particle/time-step with 768 processing cores. For the 3D electromagnetic code, a typical execution time f
              or the particle part of this code is about 280 ps/particle/time-step with 768 processing cores. For the 3D 
              Darwin code, a typical execution time for the particle part of this code is about 715 ps/particle/time-step 
              with 768 processing cores. The CPUs (2.67GHz Intel Nehalem processors) were throttled down to 1.6 GHz for 
              these benchmarks.</p>
              <p>Electrostatic:<br />
              <ul>
              	<li>1. 2D Parallel Electrostatic Spectral code:  <a href="downloads/mppic2.tar.gz">mppic2</a></li>
              	<li>2. 3D Parallel Electrostatic Spectral code:  <a href="downloads/mppic3.tar.gz">mppic3</a></li>
              </ul>
              </p>
              <p>Electromagnetic:<br />
              <ul>
              	<li>3. 2-1/2D Parallel Electromagnetic Spectral code:  <a href="downloads/mpbpic2.tar.gz">mpbpic2</a></li>
              	<li>4. 3D Parallel Electromagnetic Spectral code:  <a href="downloads/mpbpic3.tar.gz">mpbpic3</a></li>
              </ul>
              </p>
              <p>Darwin:<br />
              <ul>
              	<li>5. 2-1/2D Parallel Darwin Spectral code:  <a href="downloads/mpdpic2.tar.gz">mpdpic2</a></li>
              	<li>6. 3D Parallel Darwin Spectral code:  <a href="downloads/mpdpic3.tar.gz">mpdpic3</a></li>
              </ul>
              </p><i>Figure below: Performance as a function of the number of cores. Dashed red line is ideal scaling, blue 
              shows particle time, black shows total time. Degradation of total time is due to the all to all transpose in 
              the FFT, which for large number of cores is beginning to be dominated by message latency. The degradation is 
              considerably less than in the MPI-only case.</i></p>
              <img src="assets/img/mpmpi.jpg" align="left" width="800 px"style="padding-right: 10px;" alt="">
              <p>Figure details (click to enlarge): <a href="assets/img/timing_fmpbpic2.png">Figure 1</a>; 
              <a href="assets/img/fmppic3.jpeg">Figure 2</a>; <a href="assets/img/fmpbpic31.jpeg">Figure 3</a>; 
              <a href="assets/img/fmpdpic3.jpeg">Figure 4</a>.</p>
              <p>Want to contact the developer? Send mail to Viktor Decyk 
              at <a href="mailto:decyk@physics.ucla.edu">decyk@physics.ucla.edu</a>.</p>

            </article><!-- End blog entry -->

          </div><!-- End blog entries list -->

          <div class="col-lg-4">

            <div class="sidebar">

              <h3 class="sidebar-title">Vectorization Codes</h3>
              <p>
              <ul>
              	<li><a href="downloads/mppic2.tar.gz">mppic2</a></li>
              	<li><a href="downloads/mppic3.tar.gz">mppic3</a></li>
              	<li><a href="downloads/mpbpic2.tar.gz">mpbpic2</a></li>
              	<li><a href="downloads/mpbpic3.tar.gz">mpbpic3</a></li>
              	<li><a href="downloads/mpdpic2.tar.gz">mpdpic2</a></li>
              	<li><a href="downloads/mpdpic3.tar.gz">mpdpic3</a></li>
              </ul>
              </p>
    
              </div><!-- End sidebar recent posts-->

            </div><!-- End sidebar -->

          </div><!-- End blog sidebar -->

        </div>

      </div>
    </section><!-- End Blog Single Section -->
